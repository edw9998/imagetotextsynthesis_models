{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>ROUGE</h1>"
      ],
      "metadata": {
        "id": "o_4aTopzbfsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rouge-score"
      ],
      "metadata": {
        "id": "VChbieZuc1rj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "eayHMGjobvFH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example reference and predicted\n",
        "references = ['startseq two dogs on pavement moving toward each other endseq']\n",
        "results = ['startseq two dogs playing with each other on the pavement endseq']\n",
        "\n",
        "# Initialize scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = True)\n",
        "\n",
        "# Calculate ROUGE for each pair of reference and hypothesis\n",
        "for ref, res in zip(references, results):\n",
        "    scores = scorer.score(ref, res)\n",
        "    print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXtn0Wzxbxg3",
        "outputId": "48abe5d8-ce3e-486f-e87c-8b7a983fc64a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': Score(precision=0.7272727272727273, recall=0.8, fmeasure=0.761904761904762), 'rouge2': Score(precision=0.3, recall=0.3333333333333333, fmeasure=0.3157894736842105), 'rougeL': Score(precision=0.5454545454545454, recall=0.6, fmeasure=0.5714285714285713)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>METEOR</h1>"
      ],
      "metadata": {
        "id": "VHTJzH9SdHpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk"
      ],
      "metadata": {
        "id": "bTxeFVl-dO9c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXoTWAmLdXVW",
        "outputId": "395e64ef-3ec7-4eb3-d8c3-9c0064beff33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example reference and predicted captions\n",
        "reference = ['startseq two dogs on pavement moving toward each other endseq']\n",
        "results = ['startseq two dogs playing with each other on the pavement endseq']\n",
        "\n",
        "# Calculate METEOR for each pair\n",
        "for ref, res in zip(reference, results):\n",
        "    # Tokenize the reference and result using nltk.word_tokenize\n",
        "    ref_tokens = nltk.word_tokenize(ref)\n",
        "    res_tokens = nltk.word_tokenize(res)\n",
        "    # Pass tokenized lists\n",
        "    print(f\"METEOR: {meteor_score([ref_tokens], res_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCh2GMTRdaSP",
        "outputId": "6f33a048-0c67-4dde-8919-596ff7fa908f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR: 0.6953898514851486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>CIDEr</h1>"
      ],
      "metadata": {
        "id": "C8z11z41e5CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "CIDEr is commonly used in image captioning and can be calculated using the <i><b>pycocoevalcap</b></i> package, specifically designed for COCO-style evaluations.\n",
        "</p>"
      ],
      "metadata": {
        "id": "6b1ocOrwe-PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CIDEr = 0.6499999999999999"
      ],
      "metadata": {
        "id": "FdJGWZNW3nz2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>SPICE</h1>"
      ],
      "metadata": {
        "id": "R7_BHM-ohyWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPICE = 0.45556782"
      ],
      "metadata": {
        "id": "1iWa2_Sg361V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "SPICE evaluates the semantic content of captions.\n",
        "</p>"
      ],
      "metadata": {
        "id": "E4jycB9Kh1al"
      }
    }
  ]
}