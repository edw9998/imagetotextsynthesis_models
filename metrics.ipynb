{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>BLEU(Bilingual Evaluation Understudy)</h1>"
      ],
      "metadata": {
        "id": "5F-1cvrjWxWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Brevity Penalty(BP) <br>\n",
        "If candidate length ≥ closest reference length → BP = 1 (no penalty). <br>\n",
        "If candidate is shorter → BP < 1 to penalize short captions.\n",
        "\n",
        "2. N-gram Precision <br>\n",
        "Computes 1-gram to 4-gram overlap. <br>\n",
        "Uses clipped counts to avoid over-rewarding repeated words."
      ],
      "metadata": {
        "id": "jfLSeHxPrQb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "f9AisHYvVmah"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(tokens, n):\n",
        "    return zip(*[tokens[i:] for i in range(n)])\n",
        "\n",
        "def compute_bleu(candidate, references):\n",
        "    c = len(candidate)\n",
        "    if(c == 0):\n",
        "      return [0.0] * 4\n",
        "\n",
        "    ref_lens = [len(ref) for ref in references]\n",
        "    r = min(ref_lens, key=lambda ref_len: (abs(ref_len - c), ref_len))\n",
        "\n",
        "    # Compute brevity penalty\n",
        "    if c > r:\n",
        "        bp = 1.0\n",
        "    else:\n",
        "        bp = math.exp((1 - r) / c)\n",
        "\n",
        "    p_n = []\n",
        "    for n in range(1, 5):\n",
        "        candidate_ngrams = list(get_ngrams(candidate, n))\n",
        "        if not candidate_ngrams:\n",
        "            p_n.append(0.0)\n",
        "            continue\n",
        "\n",
        "        candidate_counts = Counter(candidate_ngrams)\n",
        "        max_ref_counts = {}\n",
        "\n",
        "        for ref in references:\n",
        "            ref_ngrams = get_ngrams(ref, n)\n",
        "            ref_counts = Counter(ref_ngrams)\n",
        "            for ngram in candidate_counts:\n",
        "                cnt = ref_counts.get(ngram, 0)\n",
        "                if ngram not in max_ref_counts or cnt > max_ref_counts[ngram]:\n",
        "                    max_ref_counts[ngram] = cnt\n",
        "\n",
        "        clipped = sum(min(count, max_ref_counts.get(ngram, 0)) for ngram, count in candidate_counts.items())\n",
        "        total = len(candidate_ngrams)\n",
        "        p_n.append(clipped / total if total != 0 else 0.0)\n",
        "\n",
        "    bleu_scores = []\n",
        "    for i in range(4):\n",
        "        n = i + 1\n",
        "        relevant_p = p_n[:n]\n",
        "        if any(p == 0 for p in relevant_p):\n",
        "            bleu = 0.0\n",
        "        else:\n",
        "            product = 1.0\n",
        "            for p in relevant_p:\n",
        "                product *= p\n",
        "            gm = product ** (1.0 / n)\n",
        "            bleu = bp * gm\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "    return bleu_scores\n",
        "\n",
        "# BLEU usage\n",
        "# resultset ex.\n",
        "candidate = \"the cat is on the mat\".split()\n",
        "# dataset values ex.\n",
        "references = [\n",
        "    \"the cat is sitting on the mat\".split(),\n",
        "    \"there is a cat on the mat\".split(),\n",
        "    \"a cat is found on the mat while sitting\".split(),\n",
        "    \"cat sits on mat\".split()\n",
        "]\n",
        "\n",
        "# bleu_scores = compute_bleu(candidate, references)\n",
        "# print(f\"BLEU-1: {bleu_scores[0]:.4f}\")\n",
        "# print(f\"BLEU-2: {bleu_scores[1]:.4f}\")\n",
        "# print(f\"BLEU-3: {bleu_scores[2]:.4f}\")\n",
        "# print(f\"BLEU-4: {bleu_scores[3]:.4f}\")"
      ],
      "metadata": {
        "id": "eGAEgbEvV2Oo"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>ROUGE-N(Recall-Oriented Understudy for Gisting Evaluation - N_Gram Recall)</h1>"
      ],
      "metadata": {
        "id": "jIvAKtqbXGvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Iterates over multiple reference sentences and calculates ROUGE scores for each.\n",
        "2. Averages precision, recall, and F1-score across all references, but focuses on the value of ROUGE-N_recall section.\n",
        "3. Supports different N-gram levels(ROUGE-1, ROUGE-2, ROUGE-3, etc.)."
      ],
      "metadata": {
        "id": "P2jr8iAsqPQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rouge-score"
      ],
      "metadata": {
        "id": "FmBB2oLuawJh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "j1qaJ40ba4Mj"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rouge_n(candidate, references, n = 1):\n",
        "    scorer = rouge_scorer.RougeScorer([f'rouge{n}'], use_stemmer=True)\n",
        "    scores = [scorer.score(ref, candidate)[f'rouge{n}'] for ref in references]\n",
        "\n",
        "    # Compute average scores over all references\n",
        "    avg_precision = sum(score.precision for score in scores) / len(scores)\n",
        "    avg_recall = sum(score.recall for score in scores) / len(scores)\n",
        "    avg_f1 = sum(score.fmeasure for score in scores) / len(scores)\n",
        "\n",
        "    return {\n",
        "        \"precision\": avg_precision,\n",
        "        \"recall\": avg_recall,\n",
        "        \"f1-score\": avg_f1\n",
        "    }\n",
        "\n",
        "# ROUGE-N usage\n",
        "# resultset ex.\n",
        "candidate_sentence = \"The cat sat on the mat.\"\n",
        "# dataset values ex.\n",
        "reference_sentences = [\n",
        "    \"The cat is sitting on the mat.\",\n",
        "    \"A cat was resting on a mat.\",\n",
        "    \"The feline was on the mat.\"\n",
        "]\n",
        "\n",
        "# rouge_1_score = calculate_rouge_n(candidate_sentence, reference_sentences, n = 1)\n",
        "# rouge_2_score = calculate_rouge_n(candidate_sentence, reference_sentences, n = 2)\n",
        "\n",
        "# Highlight on the ROUGE-N_recall final values.\n",
        "# print(\"ROUGE-1:\", rouge_1_score)\n",
        "# print(\"ROUGE-2:\", rouge_2_score)"
      ],
      "metadata": {
        "id": "F6rned4Da_Uv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>METEOR(Metric for Evaluation of Translation with Explicit ORdering)</h1>"
      ],
      "metadata": {
        "id": "Z1ahGDI2XaFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Splits sentences into tokens for better accuracy.\n",
        "2. Computes the METEOR score for each reference and then averages them."
      ],
      "metadata": {
        "id": "A87CaN9pqCZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3R5oJGuccTr1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_meteor(candidate, references):\n",
        "    # Tokenize the sentences\n",
        "    candidate_tokens = candidate.split()\n",
        "    reference_tokens = [ref.split() for ref in references]\n",
        "\n",
        "    # Compute METEOR score for each reference and average them\n",
        "    scores = [meteor_score([ref], candidate_tokens) for ref in reference_tokens]\n",
        "    avg_meteor = np.mean(scores)\n",
        "    return avg_meteor\n",
        "\n",
        "# METEOR usage\n",
        "# resultset ex.\n",
        "candidate_sentence = \"The cat sat on the mat.\"\n",
        "# dataset values ex.\n",
        "reference_sentences = [\n",
        "    \"The cat is sitting on the mat.\",\n",
        "    \"A cat sleeps and rests on a mat.\",\n",
        "    \"The feline is on the mat.\"\n",
        "]\n",
        "\n",
        "# meteor = calculate_meteor(candidate_sentence, reference_sentences)\n",
        "# print(\"METEOR Score:\", meteor)"
      ],
      "metadata": {
        "id": "TWOVakg7cn_w"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>CIDEr(Consensus-based Image Description Evaluation)</h1>"
      ],
      "metadata": {
        "id": "N6DPollMXhu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Computes n-grams(1 to 4) for the candidate and references. <br>\n",
        "2. Calculates term frequency(TF) and document frequency(DF) for candidate and references. <br>\n",
        "3. Applies TF-IDF weighting to reduce the impact of common phrases. <br>\n",
        "4. Uses cosine similarity between the TF-IDF weights of the candidate and references. <br>\n",
        "5. Applies a Gaussian penalty to balance different n-gram contributions. <br>"
      ],
      "metadata": {
        "id": "Z17GQo7Vmp6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from itertools import chain"
      ],
      "metadata": {
        "id": "UVdY5l-ukV4k"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "source": [
        "def compute_ngrams(sentence, n):\n",
        "    \"\"\"\n",
        "    Generate n-grams from a sentence.\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    return [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "def term_frequency(ngrams):\n",
        "    \"\"\"\n",
        "    Compute term frequency for n-grams.\n",
        "    \"\"\"\n",
        "    return Counter(ngrams)\n",
        "\n",
        "def compute_cider(candidate, references, n=4, sigma=6.0):\n",
        "    \"\"\"\n",
        "    Calculate CIDEr score for a candidate sentence against multiple references.\n",
        "    Args:\n",
        "        candidate(str): The generated sentence.\n",
        "        references(list of str): List of reference sentences.\n",
        "        n(int): Maximum n-gram order(default is 4).\n",
        "        sigma(float): Gaussian penalty coefficient val.(default is 6.0).\n",
        "    Returns:\n",
        "        float: CIDEr score.\n",
        "    \"\"\"\n",
        "    # Compute term frequencies(TF) for candidate(resultset)\n",
        "    candidate_tf = {i: term_frequency(compute_ngrams(candidate, i)) for i in range(1, n+1)}\n",
        "    # Compute term frequencies for references(dataset_vals)\n",
        "    reference_tfs = [{i: term_frequency(compute_ngrams(ref, i)) for i in range(1, n+1)} for ref in references]\n",
        "\n",
        "    # Compute document frequency(DF) across all references\n",
        "    df = {i: Counter() for i in range(1, n+1)}\n",
        "    for ref_tf in reference_tfs:\n",
        "        for i in range(1, n+1):\n",
        "            for ngram in ref_tf[i]:\n",
        "                df[i][ngram] += 1\n",
        "\n",
        "    # Compute CIDEr score\n",
        "    cider_score = 0.0\n",
        "    for i in range(1, n+1):\n",
        "        # Compute TF-IDF for candidate and references using shared vocabulary\n",
        "        all_ngrams = set(candidate_tf[i].keys()).union(*[ref_tf[i].keys() for ref_tf in reference_tfs])\n",
        "        candidate_tfidf = {ngram: candidate_tf[i].get(ngram, 0) * np.log(max(1, len(references) / (df[i][ngram] + 1)))\n",
        "                           for ngram in all_ngrams}\n",
        "        reference_tfidfs = []\n",
        "\n",
        "        for ref_tf in reference_tfs:\n",
        "            ref_tfidf = {ngram: ref_tf[i].get(ngram, 0) * np.log(max(1, len(references) / (df[i][ngram] + 1)))\n",
        "                         for ngram in all_ngrams}\n",
        "            reference_tfidfs.append(ref_tfidf)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        reference_vectors = [list(ref_tfidf.values()) for ref_tfidf in reference_tfidfs]\n",
        "        candidate_vector = list(candidate_tfidf.values())\n",
        "\n",
        "        if not candidate_vector or not reference_vectors:\n",
        "            continue  # Skip if no valid n-grams present\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        reference_scores = []\n",
        "        for ref_vec in reference_vectors:\n",
        "            ref_norm = np.linalg.norm(ref_vec)\n",
        "            cand_norm = np.linalg.norm(candidate_vector)\n",
        "            if ref_norm > 0 and cand_norm > 0:\n",
        "                similarity = np.dot(candidate_vector, ref_vec) / (cand_norm * ref_norm)\n",
        "                reference_scores.append(similarity)\n",
        "\n",
        "        # Average over references and apply Gaussian penalty\n",
        "        if reference_scores:\n",
        "            avg_similarity = np.mean(reference_scores)\n",
        "            cider_score += avg_similarity * np.exp(-(i - 1) ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "    return cider_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "92ikKJeklkNm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIDEr usage\n",
        "candidate_sentence = \"The cat sat on the mat.\"\n",
        "reference_sentences = [\n",
        "    \"The cat is sitting on the mat.\",\n",
        "    \"A cat was resting on a mat.\",\n",
        "    \"The feline was to be on the mat.\"\n",
        "]\n",
        "\n",
        "# cider_score = compute_cider(candidate_sentence, reference_sentences)\n",
        "# print(\"CIDEr Score:\", cider_score)"
      ],
      "metadata": {
        "id": "xCD5DrBDl_5I"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>SPICE(Semantic Propositional Image Caption Evaluation)</h1>"
      ],
      "metadata": {
        "id": "qrBBixKzXoQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Uses nltk corpus WordNet’s Wu-Palmer similarity to find semantic relations between words.\n",
        "2. This approach focuses on word-level synonyms."
      ],
      "metadata": {
        "id": "KIS2Vd1ZpQOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from itertools import product\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YypexGPXoBND"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "0BGIIxIposvm"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordnet_similarity(word1, word2):\n",
        "    \"\"\"\n",
        "    Compute similarity between two words using WordNet.\n",
        "    \"\"\"\n",
        "    synsets1 = wn.synsets(word1)\n",
        "    synsets2 = wn.synsets(word2)\n",
        "    max_sim = 0\n",
        "    for syn1, syn2 in product(synsets1, synsets2):\n",
        "        sim = syn1.wup_similarity(syn2)\n",
        "        if sim is not None:\n",
        "            max_sim = max(max_sim, sim)\n",
        "    return max_sim if max_sim > 0 else 0  # Avoid None values\n",
        "\n",
        "def compute_spice_wordnet(candidate, references):\n",
        "    \"\"\"\n",
        "    Approximate SPICE score using WordNet-based semantic similarity.\n",
        "    \"\"\"\n",
        "    candidate_words = nltk.word_tokenize(candidate.lower())\n",
        "    reference_words = [nltk.word_tokenize(ref.lower()) for ref in references]\n",
        "\n",
        "    scores = []\n",
        "    for ref_words in reference_words:\n",
        "        similarities = []\n",
        "        for cand_word in candidate_words:\n",
        "            word_similarities = [wordnet_similarity(cand_word, ref_word) for ref_word in ref_words]\n",
        "            max_word_sim = max(word_similarities) if word_similarities else 0\n",
        "            similarities.append(max_word_sim)\n",
        "\n",
        "        scores.append(np.mean(similarities) if similarities else 0)\n",
        "\n",
        "    return np.mean(scores) if scores else 0"
      ],
      "metadata": {
        "id": "cK5KD0UjoEg-"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SPICE usage\n",
        "candidate_sentence = \"The cat sat on the mat.\"\n",
        "reference_sentences = [\n",
        "    \"The cat is sitting on the mat.\",\n",
        "    \"A cat was resting on a mat.\",\n",
        "    \"The feline was on the mat.\"\n",
        "]\n",
        "\n",
        "# spice_score_wordnet = compute_spice_wordnet(candidate_sentence, reference_sentences)\n",
        "# print(\"Appr. SPICE Score(WordNet):\", spice_score_wordnet)"
      ],
      "metadata": {
        "id": "934UVD0SoLc7"
      },
      "execution_count": 100,
      "outputs": []
    }
  ]
}