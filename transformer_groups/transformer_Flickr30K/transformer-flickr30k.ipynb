{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3286.748225,"end_time":"2024-02-13T12:03:58.974272","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-13T11:09:12.226047","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"08fb596d40c9459582b1494807fd0e71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14046b07366046e28153641a6a28ef62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"245e7991500a430fbe010167aab9f750":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be9a5dcdac2c408d84c9c38305751f6e","IPY_MODEL_9d14017310b74bcc93f6b8b0849d8e45","IPY_MODEL_bb2cae27efb24b96a925f6610569ffd7"],"layout":"IPY_MODEL_14046b07366046e28153641a6a28ef62"}},"6f6dc1b78f064018aea7750dda123ee6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d14017310b74bcc93f6b8b0849d8e45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d02c7caa105e4abd9a458bd6bf03c361","max":105,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de301865142b477997b4e63af0bdd726","value":105}},"bb2cae27efb24b96a925f6610569ffd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f6dc1b78f064018aea7750dda123ee6","placeholder":"​","style":"IPY_MODEL_bda8d51d22894131b392212f7a565660","value":" 105/105 [01:05&lt;00:00,  1.44it/s]"}},"bda8d51d22894131b392212f7a565660":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be9a5dcdac2c408d84c9c38305751f6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08fb596d40c9459582b1494807fd0e71","placeholder":"​","style":"IPY_MODEL_c81b09e9dcb64791b1c0ee772333b92c","value":"100%"}},"c81b09e9dcb64791b1c0ee772333b92c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d02c7caa105e4abd9a458bd6bf03c361":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de301865142b477997b4e63af0bdd726":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":"green","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Library setup","metadata":{"papermill":{"duration":0.008044,"end_time":"2024-02-13T11:09:14.942887","exception":false,"start_time":"2024-02-13T11:09:14.934843","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nfrom keras.applications import efficientnet\nfrom keras.layers import TextVectorization\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom tqdm import tqdm_notebook\nfrom collections import Counter\n\n# By setting a random seed, we ensure that the sequence of random numbers generated during training,\n                                                  # remains the same across different runs of the code.\n# keras.utils.set_random_seed(111)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":14.820263,"end_time":"2024-02-13T11:09:29.771441","exception":false,"start_time":"2024-02-13T11:09:14.951178","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:50:47.899925Z","iopub.execute_input":"2024-03-15T14:50:47.900658Z","iopub.status.idle":"2024-03-15T14:51:07.197637Z","shell.execute_reply.started":"2024-03-15T14:50:47.900521Z","shell.execute_reply":"2024-03-15T14:51:07.196346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data and Pre-processing","metadata":{"papermill":{"duration":0.008864,"end_time":"2024-02-13T11:09:29.79816","exception":false,"start_time":"2024-02-13T11:09:29.789296","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Path to the images\nIMAGES_PATH = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\n# Path to the captions\nCAPTIONS_PATH = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n\n# Desired image dimensions\nIMAGE_SIZE = (299, 299)\n\n# Fixed length allowed for any sequence\nSEQ_LENGTH = 24\n\n# Vocabulary size\nVOCAB_SIZE = 13000\n\n# Dimension for the image embeddings and token embeddings\nEMBED_DIM = 512\n\n# Per-layer units in the feed-forward network\nFF_DIM = 512\n\n# Batch size\nBATCH_SIZE = 512\n\n# Number of epochs\nEPOCHS = 30","metadata":{"papermill":{"duration":0.016707,"end_time":"2024-02-13T11:09:29.82343","exception":false,"start_time":"2024-02-13T11:09:29.806723","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:51:07.199622Z","iopub.execute_input":"2024-03-15T14:51:07.200275Z","iopub.status.idle":"2024-03-15T14:51:07.210035Z","shell.execute_reply.started":"2024-03-15T14:51:07.200242Z","shell.execute_reply":"2024-03-15T14:51:07.208768Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loads captions (text) data and maps them to corresponding images.\ndef load_captions_data(filename):\n    with open(filename) as caption_file:\n        caption_data = caption_file.readlines()[1:]\n        caption_mapping = {}\n        text_data = []\n        images_to_skip = set()\n\n        for line in caption_data:\n            line = line.rstrip(\"\\n\")\n            # Each image is repeated five times for the five different captions.\n            # Image name and captions are separated using a comma\n            try:\n                img_name, _, caption = line.split(\"| \")\n                # There is one row in the dataset which causes ValueError when splitting.\n                # Handling the error:\n            except ValueError:\n                img_name, caption = line.split(\"| \")\n                caption = caption[4:]\n                \n            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n            # Removing caption that are either too short to too long\n            tokens = caption.strip().split()\n            if len(tokens) < 4 or len(tokens) > SEQ_LENGTH:\n                images_to_skip.add(img_name)\n                continue\n\n            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n                # A start and an end token must be added to each caption\n                caption = \"<start> \" + caption.strip() + \" <end>\"\n                text_data.append(caption)\n\n                if img_name in caption_mapping:\n                    caption_mapping[img_name].append(caption)\n                else:\n                    caption_mapping[img_name] = [caption]\n\n        for img_name in images_to_skip:\n            if img_name in caption_mapping:\n                del caption_mapping[img_name]\n\n        return caption_mapping, text_data\n\n# Splits the dataset into training, validation, and test sets\ndef train_val_split(caption_data, validation_size=0.2, test_size=0.02, shuffle=True):\n    # Getting the list of all image names\n    all_images = list(caption_data.keys())\n    \n    # Shuffle if necessary\n    if shuffle:\n        np.random.shuffle(all_images)\n    \n    train_keys, validation_keys = train_test_split(all_images, test_size=validation_size, random_state=42)\n    validation_keys, test_keys = train_test_split(validation_keys, test_size=test_size, random_state=42)\n    \n    training_data = {img_name: caption_data[img_name] for img_name in train_keys}\n    validation_data = {img_name: caption_data[img_name] for img_name in validation_keys}\n    test_data = {img_name: caption_data[img_name] for img_name in test_keys}\n\n    # Return the splits\n    return training_data, validation_data, test_data\n\n# Loading the dataset\ncaptions_mapping, text_data = load_captions_data(CAPTIONS_PATH)\n\n# Spliting the dataset\ntrain_data, validation_data, test_data = train_val_split(captions_mapping)\nprint(f\"Total number of samples: {len(captions_mapping)}\")\nprint(f\"----> Number of training samples: {len(train_data)}\")\nprint(f\"----> Number of validation samples: {len(validation_data)}\")\nprint(f\"----> Number of test samples: {len(test_data)}\")","metadata":{"papermill":{"duration":1.226977,"end_time":"2024-02-13T11:09:31.058631","exception":false,"start_time":"2024-02-13T11:09:29.831654","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:51:07.211696Z","iopub.execute_input":"2024-03-15T14:51:07.212727Z","iopub.status.idle":"2024-03-15T14:51:08.528493Z","shell.execute_reply.started":"2024-03-15T14:51:07.212692Z","shell.execute_reply":"2024-03-15T14:51:08.527254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining **the vectorizer** with custom standardization and image augmentation","metadata":{"papermill":{"duration":0.008196,"end_time":"2024-02-13T11:09:31.075695","exception":false,"start_time":"2024-02-13T11:09:31.067499","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def custom_standardization(input_string):\n    # Lowercasing all of the captions\n    lowercase = tf.strings.lower(input_string)\n    # Charecters to remove\n    strip_chars = \"!\\\"#$%&'()*+,-./:;=?@[\\]^_`{|}~1234567890\"\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\n# Defining the vectorizer\nvectorization = TextVectorization(\n    # Number of unique tokens\n    max_tokens=VOCAB_SIZE,\n    output_mode=\"int\",\n    # Maximum length of captions. Padding tokens(zeros) will be added to shorter captions.\n    output_sequence_length=SEQ_LENGTH,\n    # Standardizing the captions\n    standardize=custom_standardization)\n\n# Adapting the vectorizer with the captions\nvectorization.adapt(text_data)\n\n# Data augmentation for image data\nimage_augmentation = keras.Sequential([layers.RandomFlip(\"horizontal\"),\n                                       layers.RandomRotation(0.2),\n                                       layers.RandomContrast(0.3)])\n\n# Standardizing the text data\ntext_data = list(map(lambda x: str(custom_standardization(x).numpy())[2:-1], text_data))","metadata":{"papermill":{"duration":28.71472,"end_time":"2024-02-13T11:09:59.798883","exception":false,"start_time":"2024-02-13T11:09:31.084163","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:51:08.530887Z","iopub.execute_input":"2024-03-15T14:51:08.531249Z","iopub.status.idle":"2024-03-15T14:51:39.055072Z","shell.execute_reply.started":"2024-03-15T14:51:08.531221Z","shell.execute_reply":"2024-03-15T14:51:39.053595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing some of the images along with the corresponding captions","metadata":{"papermill":{"duration":0.008302,"end_time":"2024-02-13T11:09:59.817172","exception":false,"start_time":"2024-02-13T11:09:59.80887","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def visualaization(data, num_of_images):\n    count = 1\n    fig = plt.figure(figsize=(10,20))\n    for filename in list(data.keys())[100:100+num_of_images]:\n        captions = data[filename]\n        image_load = load_img(filename, target_size=(199,199,3))\n\n        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        count += 1\n\n        ax = fig.add_subplot(num_of_images,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,len(captions))\n        for i, caption in enumerate(captions):\n            ax.text(0,i,caption,fontsize=20)\n        count += 1\n    plt.show()\n    \nvisualaization(train_data, 7)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.46079,"end_time":"2024-02-13T11:10:01.286375","exception":false,"start_time":"2024-02-13T11:09:59.825585","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:51:39.056757Z","iopub.execute_input":"2024-03-15T14:51:39.057173Z","iopub.status.idle":"2024-03-15T14:51:40.567544Z","shell.execute_reply.started":"2024-03-15T14:51:39.05714Z","shell.execute_reply":"2024-03-15T14:51:40.566164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def captions_length(data):\n    plt.figure(figsize=(15, 7), dpi=300)\n    sns.set_style('darkgrid')\n    sns.histplot(x=[len(x.split(' ')) for x in data], kde=True, binwidth=1) \n    plt.title('Captions length histogram', fontsize=15, fontweight='bold')\n    plt.xticks(fontweight='bold')\n    plt.yticks(fontweight='bold')\n    plt.xlabel('Length', fontweight='bold')\n    plt.ylabel('Freaquency', fontweight='bold')\n    plt.show()\n    \ncaptions_length(text_data)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.742406,"end_time":"2024-02-13T11:10:03.051751","exception":false,"start_time":"2024-02-13T11:10:01.309345","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:51:40.569317Z","iopub.execute_input":"2024-03-15T14:51:40.57008Z","iopub.status.idle":"2024-03-15T14:51:42.575048Z","shell.execute_reply.started":"2024-03-15T14:51:40.570039Z","shell.execute_reply":"2024-03-15T14:51:42.573594Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def word_occurrences(data):\n    # Combining all sentences into a single string\n    all_text = ' '.join(data)\n    all_text = all_text.replace('a ', '')\n    all_text = all_text.replace('<start> ', '')\n    all_text = all_text.replace('<end> ', '')\n    # Splitting the text into words and count occurrences\n    word_counts = Counter(all_text.split())\n\n    words = list(word_counts.keys())[:30]\n    values = list(word_counts.values())[:30]\n\n    # Normalize values to be between 0 and 1\n    normalized_values = np.array(values) / np.max(values)\n    colors = np.array(['rgba(30, 58, 138, {})'.format(0.4 + 0.5 * (value)) for value in normalized_values])\n\n    fig = go.Figure(data=[go.Pie(labels=words, values=values, hole=.6, marker=dict(colors=colors), textinfo='label')])\n\n    fig.update_layout(title_text='Word occurrences in captions (except for letter \\'a\\')', title_font=dict(size=23, family='Balto'))\n\n    fig.show()\n    \nword_occurrences(text_data)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.767351,"end_time":"2024-02-13T11:10:03.847587","exception":false,"start_time":"2024-02-13T11:10:03.080236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:51:42.576885Z","iopub.execute_input":"2024-03-15T14:51:42.577423Z","iopub.status.idle":"2024-03-15T14:51:43.57441Z","shell.execute_reply.started":"2024-03-15T14:51:42.577366Z","shell.execute_reply":"2024-03-15T14:51:43.573002Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Setup for Model Input**\n##### **Parallelization** Involves **breaking down a task** into **smaller subtasks** that can be **executed in parallel**.\n##### **'tf.data.AUTOTUNE'** determines the **optimal** level of **parallelism**.","metadata":{"papermill":{"duration":0.027074,"end_time":"2024-02-13T11:10:03.9035","exception":false,"start_time":"2024-02-13T11:10:03.876426","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Processes the images\ndef decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMAGE_SIZE)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    return img\n\n\ndef process_input(img_path, captions):\n    # Processed images: (None, 299, 299, 3), Vectorized captions: (None, None, 25)\n    return decode_and_resize(img_path), vectorization(captions)\n\n# Prepares the dataset\ndef make_dataset(images, captions):\n    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n    dataset = dataset.shuffle(BATCH_SIZE * 8)\n    dataset = dataset.map(process_input, num_parallel_calls=tf.data.AUTOTUNE)\n    # Prefetching the next batch of data based on available resources while the current batch is being processed.\n    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n    return dataset\n\n\n# Making the datasets by passing the list of images and the list of corresponding captions\ntrain_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\nvalidation_dataset = make_dataset(list(validation_data.keys()), list(validation_data.values()))","metadata":{"papermill":{"duration":0.938468,"end_time":"2024-02-13T11:10:04.869507","exception":false,"start_time":"2024-02-13T11:10:03.931039","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T14:51:43.576552Z","iopub.execute_input":"2024-03-15T14:51:43.577106Z","iopub.status.idle":"2024-03-15T14:51:44.708109Z","shell.execute_reply.started":"2024-03-15T14:51:43.577062Z","shell.execute_reply":"2024-03-15T14:51:44.70668Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-15T14:51:44.710285Z","iopub.execute_input":"2024-03-15T14:51:44.710911Z","iopub.status.idle":"2024-03-15T14:51:44.718935Z","shell.execute_reply.started":"2024-03-15T14:51:44.71085Z","shell.execute_reply":"2024-03-15T14:51:44.717439Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Defining** the **Image Captioning** Model\n\n> ### **CNN** (feature extraction)\n> * Pre-Trained model: EfficientNetB0\n\n> ### **Encoder** (embedded image features)\n> * Layer Normalization Layer\n> * Dense Layer\n> * Multi-Head Attention Layer\n> * Layer Normalization Layer\n\n> ### **Decoder** (embedded captions)\n> * Positional Embedding Layer (input embedding + positional encoding)\n> * Mask (enables the model to ignore both padding tokens and future tokens)\n> * Multi-Head Attention Layer\n> * Layer Normalization Layer\n> * Multi-Head Cross-Attention Layer\n> * Layer Normalization Layer\n> * Feed Forward Layer\n> * Layer Normalization Layer\n\n> ### **Output**\n> * Dense Layer + Softmax activation function","metadata":{"papermill":{"duration":0.030136,"end_time":"2024-02-13T11:10:04.929503","exception":false,"start_time":"2024-02-13T11:10:04.899367","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_cnn_model():\n    base_model = efficientnet.EfficientNetB0(\n        input_shape=(*IMAGE_SIZE, 3),\n        include_top=False, # Removing the prediction layers\n        weights=\"imagenet\")\n    # Freezing the model's weights\n    base_model.trainable = False\n    base_model_out = base_model.output\n    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n    cnn_model = keras.models.Model(base_model.input, base_model_out)\n    return cnn_model\n\n\nclass TransformerEncoderBlock(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=0.0)\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n\n    def call(self, inputs, training, mask=None):\n        inputs = self.layernorm_1(inputs)\n        inputs = self.dense_1(inputs)\n        attention_output_1 = self.attention_1(query=inputs,\n                                              value=inputs,\n                                              key=inputs,\n                                              attention_mask=None,\n                                              training=training)\n        out_1 = self.layernorm_2(inputs + attention_output_1)\n        return out_1\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1) # Positional encoding\n        embedded_tokens = self.token_embeddings(inputs) # Input embedding\n        embedded_tokens = embedded_tokens * self.embed_scale\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions # Positional embedding\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n\nclass TransformerDecoderBlock(layers.Layer):\n    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.ff_dim = ff_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=0.1)\n        self.cross_attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=0.1)\n        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n        self.ffn_layer_2 = layers.Dense(embed_dim)\n\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n\n        self.embedding = PositionalEmbedding(embed_dim=EMBED_DIM,\n                                             sequence_length=SEQ_LENGTH,\n                                             vocab_size=VOCAB_SIZE,)\n        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n\n        self.dropout_1 = layers.Dropout(0.3)\n        self.dropout_2 = layers.Dropout(0.5)\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, training, mask=None):\n        inputs = self.embedding(inputs)\n        causal_mask = self.get_causal_attention_mask(inputs)\n        \n        # If the mask is not None, it means that padding tokens are present in the input sequence.\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n            # Masking both padding tokens and future tokens\n            combined_mask = tf.minimum(combined_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(query=inputs,\n                                              value=inputs,\n                                              key=inputs,\n                                              attention_mask=combined_mask,\n                                              training=training)\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n        \n        # Note that the lengths of the inputs are different and cross-attention handles that.\n        cross_attention_output_2 = self.cross_attention_2(query=out_1,\n                                              value=encoder_outputs,\n                                              key=encoder_outputs,\n                                              attention_mask=padding_mask,\n                                              training=training)\n        out_2 = self.layernorm_2(out_1 + cross_attention_output_2)\n\n        ffn_out = self.ffn_layer_1(out_2)\n        ffn_out = self.dropout_1(ffn_out, training=training)\n        ffn_out = self.ffn_layer_2(ffn_out)\n\n        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n        ffn_out = self.dropout_2(ffn_out, training=training)\n        \n        preds = self.out(ffn_out)\n        return preds\n    \n    # Masks future tokens\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat([tf.expand_dims(batch_size, -1),tf.constant([1, 1], dtype=tf.int32)],axis=0)\n        return tf.tile(mask, mult)\n\n\nclass ImageCaptioningModel(keras.Model):\n    def __init__(self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None):\n        super().__init__()\n        self.cnn_model = cnn_model\n        self.encoder = encoder\n        self.decoder = decoder\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n        self.num_captions_per_image = num_captions_per_image\n        self.image_aug = image_aug\n        \n        print()\n        print(f'CNN input shape: {cnn_model.input_shape}')\n        print(f'CNN output shape: {cnn_model.output_shape}', end='\\n'*2)\n        print(f'Encoder input ---> Dense layer shape: {cnn_model.output_shape} ---> (None, {cnn_model.output_shape[1]}, {EMBED_DIM})')\n        print(f'Encoder output shape: (None, {cnn_model.output_shape[1]}, {EMBED_DIM})', end='\\n'*2)\n        print(f'Decoder input 1 (Caption) ---> Positional Embedding shape: (None, {SEQ_LENGTH-1}) ---> (None, {SEQ_LENGTH-1}, {EMBED_DIM})')\n        print(f'Decoder input 2 (Embedded image features) shape: (None, {cnn_model.output_shape[1]}, {EMBED_DIM})')\n        print(f'Decoder output (MH Cross-Attention) shape: (None, {SEQ_LENGTH-1}, {EMBED_DIM})')\n        print(f'Decoder prediction (Dense layer) shape: (None, {SEQ_LENGTH-1}, {VOCAB_SIZE})')\n        \n    \n    # Calculates the loss, taking into account a mask to handle padding.\n    def calculate_loss(self, y_true, y_pred, mask):\n        loss = self.loss(y_true, y_pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n    \n    # Calculates the accuracy, taking into account a mask to handle padding.\n    def calculate_accuracy(self, y_true, y_pred, mask):\n        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n        accuracy = tf.math.logical_and(mask, accuracy)\n        accuracy = tf.cast(accuracy, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n\n    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n        encoder_out = self.encoder(img_embed, training=training)\n        batch_seq_inp = batch_seq[:, :-1]\n        batch_seq_true = batch_seq[:, 1:]\n        # Creating a binary mask where 1 indicates a valid token, and 0 indicates padding.\n        mask = tf.math.not_equal(batch_seq_true, 0)\n        batch_seq_pred = self.decoder(batch_seq_inp, encoder_out, training=training, mask=mask)\n        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n        return loss, acc\n    \n    # Iterates through each caption for the given image, computes loss and accuracy, updates weights, and trackers.\n    def train_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n        \n        # Applies image augmentation if image_aug is provided.\n        if self.image_aug:\n            batch_img = self.image_aug(batch_img)\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the five captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            with tf.GradientTape() as tape:\n                loss, acc = self._compute_caption_loss_and_acc(img_embed, batch_seq[:, i, :], training=True)\n\n                # 3. Update loss and accuracy\n                batch_loss += loss\n                batch_acc += acc\n\n            # 4. Get the list of all the trainable weights\n            train_vars = (self.encoder.trainable_variables + self.decoder.trainable_variables)\n\n            # 5. Get the gradients\n            grads = tape.gradient(loss, train_vars)\n\n            # 6. Update the trainable weights\n            self.optimizer.apply_gradients(zip(grads, train_vars))\n\n        # 7. Update the trackers\n        batch_acc /= float(self.num_captions_per_image)\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 8. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(),\n                \"acc\": self.acc_tracker.result()}\n    \n    # Similar to train_step but without updating weights.\n    def test_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the five captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            loss, acc = self._compute_caption_loss_and_acc(img_embed, batch_seq[:, i, :], training=False)\n\n            # 3. Update batch loss and batch accuracy\n            batch_loss += loss\n            batch_acc += acc\n\n        batch_acc /= float(self.num_captions_per_image)\n\n        # 4. Update the trackers\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 5. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(),\n                \"acc\": self.acc_tracker.result()}\n\n    @property\n    def metrics(self):\n        # We must list the metrics here so the `reset_states()` can be,\n                                                  # called automatically.\n        return [self.loss_tracker, self.acc_tracker]\n\n\ncnn_model = get_cnn_model()\nencoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=2)\ndecoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=3)\ncaption_model = ImageCaptioningModel(cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation)","metadata":{"execution":{"iopub.execute_input":"2024-02-13T11:10:04.988239Z","iopub.status.busy":"2024-02-13T11:10:04.9879Z","iopub.status.idle":"2024-02-13T11:10:08.019189Z","shell.execute_reply":"2024-02-13T11:10:08.01827Z"},"papermill":{"duration":3.063325,"end_time":"2024-02-13T11:10:08.021323","exception":false,"start_time":"2024-02-13T11:10:04.957998","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training the model\n##### **SparseCategoricalCrossentropy** is **commonly used** for tasks where **each target sequence is a sequence of integers** representing the **class indices**.\n","metadata":{"papermill":{"duration":0.030207,"end_time":"2024-02-13T11:10:08.081643","exception":false,"start_time":"2024-02-13T11:10:08.051436","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Defining the loss function\ncross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n\n# EarlyStopping criteria\n# Training will stop if there is no improvement in the validation loss for 3 consecutive epochs.\nearly_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\n\n# Learning Rate Scheduler for the optimizer\nclass LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, post_warmup_learning_rate, warmup_steps):\n        super().__init__()\n        self.post_warmup_learning_rate = post_warmup_learning_rate\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        global_step = tf.cast(step, tf.float32)\n        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n        warmup_progress = global_step / warmup_steps\n        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n        return tf.cond(\n            global_step < warmup_steps,\n            lambda: warmup_learning_rate,\n            lambda: self.post_warmup_learning_rate)\n    \n# Creating a learning rate schedule\nnum_train_steps = len(train_dataset) * EPOCHS\nnum_warmup_steps = num_train_steps // 15\nlr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n\n# Compiling the model\ncaption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n\n# Training the model\nhistory = caption_model.fit(train_dataset, epochs=EPOCHS, validation_data=validation_dataset, callbacks=[early_stopping])","metadata":{"execution":{"iopub.execute_input":"2024-02-13T11:10:08.139523Z","iopub.status.busy":"2024-02-13T11:10:08.138839Z","iopub.status.idle":"2024-02-13T12:02:42.410778Z","shell.execute_reply":"2024-02-13T12:02:42.409713Z"},"papermill":{"duration":3154.302693,"end_time":"2024-02-13T12:02:42.412956","exception":false,"start_time":"2024-02-13T11:10:08.110263","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 7), dpi=200)\nsns.set_style('whitegrid')\nplt.plot([x+1 for x in range(len(history.history['loss']))], history.history['loss'], color='#004EFF', marker='o')\nplt.plot([x+1 for x in range(len(history.history['loss']))], history.history['val_loss'], color='#00008B', marker='h')\nplt.title('Train VS Validation', fontsize=15, fontweight='bold')\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\nplt.xlabel('Epoch', fontweight='bold')\nplt.ylabel('Loss', fontweight='bold')\nplt.legend(['Train Loss', 'Validation Loss'], loc='best')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-13T12:02:42.605246Z","iopub.status.busy":"2024-02-13T12:02:42.604389Z","iopub.status.idle":"2024-02-13T12:02:43.130096Z","shell.execute_reply":"2024-02-13T12:02:43.129179Z"},"papermill":{"duration":0.623041,"end_time":"2024-02-13T12:02:43.132603","exception":false,"start_time":"2024-02-13T12:02:42.509562","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\n> ##### **Caption Generation:** At **each time step**, **the decoder takes** as input **the embedded image features** along with **the decoded captions** (starting with <start\\> at the first time step), **predicting the probabilities** of **the next word**.\n> ##### **Greedy algorithm:** To select  **the best caption**, **the greedy algorithm** is employed. This method, chooses **the most probable word** at each time step and **appends it** to the decoded captions **until** the selected word is the **<end\\>** token, **or the length** of the decoded captions exceeds  **the maximum sequence length**.\n> ##### **Evaluation** is performed using the **BLEU score**\n> ##### **Visualization:** The function visualization() plots the **images along with their corresponding actual and predicted captions, accompanied by 4 BLEU scores**.","metadata":{"papermill":{"duration":0.097723,"end_time":"2024-02-13T12:02:43.330502","exception":false,"start_time":"2024-02-13T12:02:43.232779","status":"completed"},"tags":[]}},{"cell_type":"code","source":"vocab = vectorization.get_vocabulary()\nINDEX_TO_WORD = {idx: word for idx, word in enumerate(vocab)}\nMAX_DECODED_SENTENCE_LENGTH = SEQ_LENGTH - 1\ntest_images = list(test_data.keys())\n\ndef greedy_algorithm(image):\n    # Read the image from the disk\n    image = decode_and_resize(image)\n\n    # Pass the image to the CNN\n    image = tf.expand_dims(image, 0)\n    image = caption_model.cnn_model(image)\n\n    # Pass the image features to the Transformer encoder\n    encoded_img = caption_model.encoder(image, training=False)\n\n    # Generate the caption using the Transformer decoder\n    decoded_caption = \"<start> \"\n    for i in range(MAX_DECODED_SENTENCE_LENGTH):\n        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n        mask = tf.math.not_equal(tokenized_caption, 0)\n        predictions = caption_model.decoder(tokenized_caption, encoded_img, training=False, mask=mask)\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = INDEX_TO_WORD[sampled_token_index]\n        if sampled_token == \"<end>\":\n            break\n        decoded_caption += \" \" + sampled_token\n\n    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n    \n    return decoded_caption","metadata":{"execution":{"iopub.execute_input":"2024-02-13T12:02:43.52781Z","iopub.status.busy":"2024-02-13T12:02:43.527174Z","iopub.status.idle":"2024-02-13T12:02:43.569373Z","shell.execute_reply":"2024-02-13T12:02:43.568487Z"},"papermill":{"duration":0.140957,"end_time":"2024-02-13T12:02:43.57143","exception":false,"start_time":"2024-02-13T12:02:43.430473","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generating captions\ngenerated_captions = {}\n\npbar = tqdm_notebook(total=len(test_data), position=0, leave=True, colour='green')\nfor image_id in test_data:\n    cap = greedy_algorithm(image_id)\n    generated_captions[image_id] = cap\n    pbar.update(1)\n    \npbar.close()","metadata":{"execution":{"iopub.execute_input":"2024-02-13T12:02:43.765244Z","iopub.status.busy":"2024-02-13T12:02:43.764919Z","iopub.status.idle":"2024-02-13T12:03:49.458147Z","shell.execute_reply":"2024-02-13T12:03:49.457246Z"},"papermill":{"duration":65.792684,"end_time":"2024-02-13T12:03:49.460634","exception":false,"start_time":"2024-02-13T12:02:43.66795","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculates BLEU score of predictions\ndef BLEU_score(actual, predicted):\n    # Standardizing the actual captions\n    processed_actual = []\n    for i in actual:\n        cap = [INDEX_TO_WORD[x] for x in vectorization(i).numpy() if INDEX_TO_WORD[x] != '']\n        cap = ' '.join(cap)\n        processed_actual.append(cap)\n    \n    # Calculating the BLEU score by comparing the predicted caption with five actual captions.\n    b1=corpus_bleu(processed_actual, predicted, weights=(1.0, 0, 0, 0))\n    b2=corpus_bleu(processed_actual, predicted, weights=(0.5, 0.5, 0, 0))\n    b3=corpus_bleu(processed_actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n    b4=corpus_bleu(processed_actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return [\n        (f'BLEU-4: {round(b4, 5)}'),\n        (f'BLEU-3: {round(b3, 5)}'),\n        (f'BLEU-2: {round(b2, 5)}'),\n        (f'BLEU-1: {round(b1, 5)}'),\n        (f'Generated Caption: {predicted[0]}')\n    ]","metadata":{"execution":{"iopub.execute_input":"2024-02-13T12:03:49.662009Z","iopub.status.busy":"2024-02-13T12:03:49.661212Z","iopub.status.idle":"2024-02-13T12:03:49.669307Z","shell.execute_reply":"2024-02-13T12:03:49.668447Z"},"papermill":{"duration":0.111201,"end_time":"2024-02-13T12:03:49.67113","exception":false,"start_time":"2024-02-13T12:03:49.559929","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing some of the **test images** along with their corresponding **generated captions**","metadata":{"papermill":{"duration":0.1025,"end_time":"2024-02-13T12:03:49.885","exception":false,"start_time":"2024-02-13T12:03:49.7825","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def visualization(data, generated_captions, evaluator, num_of_images):\n    keys = list(data.keys()) # List of all test images\n    images = [np.random.choice(keys) for i in range(num_of_images)] # Randomly selected images\n    \n    count = 1\n    fig = plt.figure(figsize=(6,20))    \n    for filename in images:\n        actual_cap = data[filename]\n        actual_cap = [x.replace(\"<start> \", \"\") for x in actual_cap] # Removing the start token\n        actual_cap = [x.replace(\" <end>\", \"\") for x in actual_cap] # Removing the end token\n        \n        caption = generated_captions[filename]\n        # Getting the bleu score\n        caps_with_score = evaluator(actual_cap, [caption]*(len(actual_cap)))\n    \n        image_load = load_img(filename, target_size=(199,199,3))\n        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        count += 1\n\n        ax = fig.add_subplot(num_of_images,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,len(caps_with_score))\n        for i, text in enumerate(caps_with_score):\n            ax.text(0,i,text,fontsize=10)\n        count += 1\n    plt.show()\n    \nvisualization(test_data, generated_captions, BLEU_score, 7)","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-13T12:03:50.128149Z","iopub.status.busy":"2024-02-13T12:03:50.127686Z","iopub.status.idle":"2024-02-13T12:03:52.480606Z","shell.execute_reply":"2024-02-13T12:03:52.479717Z"},"papermill":{"duration":2.50801,"end_time":"2024-02-13T12:03:52.489716","exception":false,"start_time":"2024-02-13T12:03:49.981706","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captions_length(list(generated_captions.values()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-13T12:03:52.707555Z","iopub.status.busy":"2024-02-13T12:03:52.707215Z","iopub.status.idle":"2024-02-13T12:03:53.538546Z","shell.execute_reply":"2024-02-13T12:03:53.537571Z"},"papermill":{"duration":0.943181,"end_time":"2024-02-13T12:03:53.541151","exception":false,"start_time":"2024-02-13T12:03:52.59797","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_occurrences(list(generated_captions.values()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-13T12:03:53.76306Z","iopub.status.busy":"2024-02-13T12:03:53.762259Z","iopub.status.idle":"2024-02-13T12:03:53.777367Z","shell.execute_reply":"2024-02-13T12:03:53.776562Z"},"papermill":{"duration":0.127539,"end_time":"2024-02-13T12:03:53.779128","exception":false,"start_time":"2024-02-13T12:03:53.651589","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}